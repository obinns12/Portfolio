gradschool <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")

#view the first few rows of summary statistics
View(gradschool)
head(gradschool)
summary(gradschool)

#display standard deviations for each numeric variable
sapply(gradschool,sd)

#convert rank to a factor since it is a categorical variable
gradschool$rank <- as.factor(gradschool$rank)

#the response variable (admit) is binary, so we use glm() with family = "binomial"
logit_model <- glm(admit ~ gre + gpa + rank, data = gradschool, family = "binomial")
summary(logit_model)

#exponentiating the coefficients gives us the odds ration
exp(coef(logit_model))

#McFaddens R squared
if(!require(pscl)) install.packages("pscl", dependencies = TRUE)
library(pscl)

#pR2() function gives multiple pseudo-R^2 values
# <0.10 poor model
# 0.10-0.199 modest model
# 0.20-0.399 good model
# 0.40-0.499 great model
# >= 0.50 excellent model (rare)
pR2(logit_model)

#McFaddens R^2 specifically:
McFadden_R2 <- 1 - (logLik(logit_model)/logLik(update(logit_model, . ~ 1)))
McFadden_R2

#make predictions (obtain predicted probabilities of admission for each observation)
gradschool$Predicted <- predict(logit_model, type = "response")

#percent change in odds
percent_change <- (exp(coef(logit_model))- 1) * 100
round(percent_change, 2)

#evaluate model performance
#create a simple confusion matrix using cutoff 0.4
#(if predicted >0.4, we predict "admitted")
table(observed = gradschool$admit, Predicted = gradschool$Predicted > 0.5)
table(observed = gradschool$admit, Predicted = gradschool$Predicted > 0.4)

#install ROCR
if(!require(ROCR)) install.packages("ROCR", dependencies = TRUE)
library(ROCR)

# Generate prediction and performance objects
ROCRpred <- prediction(gradschool$Predicted, gradschool$admit)
ROCRperf <- performance(ROCRpred, 'tpr', 'fpr')
acc_perf <- performance(ROCRpred, "acc")         # Accuracy at same thresholds


# Build a table of threshold metrics
roc_tbl <- data.frame(
  threshold   = unlist(ROCRperf@alpha.values),
  tpr         = unlist(ROCRperf@y.values),
  fpr         = unlist(ROCRperf@x.values),
  accuracy    = unlist(acc_perf@y.values)
)

# Clean-up + derived metrics
roc_tbl <- subset(roc_tbl, !is.na(threshold))     # drop NAs that can appear at extremes
roc_tbl <- roc_tbl[order(-roc_tbl$threshold), ]   # sort from high to low threshold
roc_tbl$specificity <- 1 - roc_tbl$fpr
roc_tbl$youdenJ     <- roc_tbl$tpr - roc_tbl$fpr  # useful for picking a cutoff

head(roc_tbl, 10)


#Plot ROC curve with cutoff points displayed
plot(ROCRperf, colorize = TRUE,
     print.cutoffs.at = seq(0.1, by = 0.1, length = 10),
     text.adj = c(-0.2, 1.7))

######Other Notes:
#Youden's J
# J = Sensitivity + Specificity - 1
#Sensitivity: the power to predict a 1 (true positive)
#Specificity: the power to predict a 0 (true negative)
#__% of those we predict a 1/0 (admit/denied) we correctly predicted
#TPR = True Positivity Rate (Sensitivity)
#FPR = False Positivity Rate

#using pROC to confirm best cutoff by Youden's J and show nearby points
if(!require(pROC)) install.packages("pROC", dependencies = TRUE)
library(pROC)

roc_obj <- roc(gradschool$admit, gradschool$Predicted, quiet = TRUE)
best_j <- coords(roc_obj, "best", ret = c("threshold", "sensitivity", "specificity"), transpose = FALSE)
best_j

#inspect around 0.3, 0.4, 0.5, to show the trade-offs
pts <- as.data.frame(coords(roc_obj, x = c(0.3, 0.4, 0.5),
                            ret = c("threshold", "sensitivity", "specificity", "accuracy"),
                            transpose = FALSE))
pts$one_minus_spec <- 1 - pts$specificity
pts

